{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Classification: Convolutional Neural Networks\n",
    "\n",
    "\n",
    "## 1. The Problem\n",
    "\n",
    "The goal is to classify audio spectrograms into one of two classes.\n",
    "\n",
    "So although it's audio, we have the images of the audio.\n",
    "\n",
    "\n",
    "## 2. The Dataset\n",
    "\n",
    "The dataset contains spectrograms of Stephen Colbert and Conan O'Brien speaking. This dataset has been put together by Sean M. Tracey.\n",
    "\n",
    "The source videos from which the audio samples have been extracted, and the spectrograms generated from are:\n",
    "\n",
    "Stephen Colbert:\n",
    "- https://www.youtube.com/watch?v=U2_52Dj6DsI\n",
    "- https://www.youtube.com/watch?v=m6tiaooiIo0\n",
    "\n",
    "Conan O'Brien:\n",
    "- https://www.youtube.com/watch?v=KmDYXaaT9sA\n",
    "- https://www.youtube.com/watch?v=_q471WB5Tgw\n",
    "- https://www.youtube.com/watch?v=DtJ28qOEG1g\n",
    "\n",
    "The audio content is from different time periods across 2 decades. Once extracted from the videos, each audio file is divided into 250ms clips. These clips are then analysed to generate spectrograms that can be classified by a CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "We'll be using tensorflow 2.0 to construct our neural network. Make sure you have the following packages. The version may differ but be careful about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.7.5\n",
      "IPython 7.10.0\n",
      "\n",
      "numpy 1.17.4\n",
      "scipy 1.3.1\n",
      "tensorflow 2.0.0\n",
      "matplotlib 3.1.2\n",
      "tqdm 4.39.0\n",
      "\n",
      "compiler   : Clang 4.0.1 (tags/RELEASE_401/final)\n",
      "system     : Darwin\n",
      "release    : 19.0.0\n",
      "machine    : x86_64\n",
      "processor  : i386\n",
      "CPU cores  : 16\n",
      "interpreter: 64bit\n",
      "Git hash   : e002afdb781c2538f12af576a44b04aefe95ff13\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%watermark -v -m -p numpy,scipy,tensorflow,matplotlib,tqdm  -g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the training data\n",
    "\n",
    "The training data consists of 2394 JPGs (spectrograms) of the raw audio data from the processed video files.\n",
    "\n",
    "Once we have the data, we divide it into 3 different categories:\n",
    "\n",
    "- training (75%)\n",
    "- test (20%)\n",
    "- validation (5%)\n",
    "\n",
    "The `training` data will be used to train our model on the different patterns in Stephen Colbert and Conan O'Brien's speech. \n",
    "\n",
    "The `test` data is used by the model to track how well it's doing in the current epoch. \n",
    "\n",
    "The `validation` data is not used by the model, but is reserved by us to run some code later on in this notebook.\n",
    "\n",
    "You can retrieve these files from a DropBox folder and extract them to a working directory for our script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p ./data\n",
    "# !curl -L --output ./data/audio_data.zip https://www.dropbox.com/s/rbywvpnd7h3d5ra/audio_data.zip?dl=1\n",
    "# !unzip -o ./data/audio_data.zip -d ./data\n",
    "# !ls -la && ls -la ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries\n",
    "\n",
    "Now we import the required dependencies we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy import misc\n",
    "import numpy as np\n",
    "\n",
    "# And all the required tensorflow libraries (using tensorflow 2.0)\n",
    "import tensorflow as tf2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking Available Files\n",
    "\n",
    "This is not strictly neccesary in our notebook but it is handy little bit of code that we can use to define our training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_class_names = [dI for dI in os.listdir('data/train') if os.path.isdir(os.path.join('data/train', dI))]\n",
    "labelled_classes = {}\n",
    "\n",
    "for class_name in labelled_class_names:\n",
    "    trainingPath = 'data/train/' + class_name\n",
    "    testPath = 'data/test/' + class_name\n",
    "\n",
    "    labelled_classes[class_name] = {}\n",
    "    labelled_classes[class_name]['training'] = len([f for f in listdir(trainingPath) \n",
    "                                                    if isfile(join (trainingPath, f))])\n",
    "    labelled_classes[class_name]['test'] = len([f for f in listdir(testPath) \n",
    "                                                if isfile(join(testPath, f))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colbert': {'training': 957, 'test': 180},\n",
       " 'conan': {'training': 957, 'test': 180}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "We set the parameters that our generators will use to decide how to train our model.\n",
    "\n",
    "1. Create variables that we can use to point our code to the locations of our training and test data on our filesystem (where we unzipped our data files in the early steps of this notebook)\n",
    "\n",
    "2. Create a variable which lists the number of training + test files in our dataset. This is used later on in our dataset generators (some natty code which handles all the messy business of passing our training/test data to our model) to determine the number of training steps are needed for each epoch.\n",
    "\n",
    "3. Set the number of epochs we want our training cycle to have. We don't want to have too many epochs as we have quite a small dataset, if we have too many epochs, our model will be trainined to recognise our dataset, not the speech patterns of the people we're trying to identify. This would negate our models efficacy in any environment outside of this notebook / dataset. 3 epochs on a dataset this size should give us a confidence of ~90% - 95%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 110, 110\n",
    "\n",
    "train_data_dir = 'data/train'           # Folder used to train our model\n",
    "test_data_dir = 'data/test' # Folder used to validate the model\n",
    "\n",
    "nb_train_samples = labelled_classes[labelled_class_names[0]]['training']    # Number of files to train our model\n",
    "nb_test_samples = labelled_classes[labelled_class_names[0]]['test']         # Number of files to validate our model\n",
    "\n",
    "epochs = 3      # Iterations (epochs) that our data will pass through the model to train it.\n",
    "batch_size = 16 # How many files from our training / test dataset will include in our batch at training time\n",
    "\n",
    "# Depending on the backend Keras is running with, we either include the dimensions of our imnage before we include the number of channels\n",
    "# This code sets the shape according to the backend included in the script.\n",
    "if K.image_data_format() == 'channels_first': \n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing Our Model\n",
    "\n",
    "It's time to start building our neural network!\n",
    "\n",
    "In this next cell, we set the `model` variable. Here, we're telling our script that we want to create a sequential neural network, that is, a network which layers are triggered in tandem with each other, one layer after the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking down our neural network\n",
    "\n",
    "Let's break down our layers line by line.\n",
    "\n",
    "First, we have `model.add(Conv2D(32, (3, 3), input_shape=input_shape))` which, as the code suggests, creates a 2D convolutional layer. A convolutional layers job is to take an input (in this case, our image) and to essentially move around areas of it (convolve around) and tally up all of the values in that section of the image. Imagine you have a photo and a small square, if you were to place the square over a part of the image and tally up all of the values of the image inside the grid you would get a single number. If you repeat this process for all of the remaining parts of the image, you would end up with a matrix of numbers which you would then pass on to the next layer of our network. You can think of it as a filter. Our image goes in, the **_key features_** of our image are what is output, and that's what's important - we're hoping (and hope is the right word here) that our convolutional layer will be able to pick out the key features of our spectrograms as we show it more and more of them. This layer has `32` outputs, so we're greatly reducing the amount of information (A 110x110 image has 12,100 data points) passing from our source to the next layer of our network.\n",
    "\n",
    "Next up, we have `model.add(Activation('relu'))`. This creates a layer of densely connected tensors (that is, a series of tensors which connects to every input and output before it) that will be activated with the **ReLu** function. The **ReLu** activation function is generally the go-to choice of activation functions when building convolutional neural networks. It's uncomplicated (it ignores negative input values), so it's computationally cheap to process, meaning we can usually train our networks faster with results comparative to networks built with other activation functions.\n",
    "\n",
    "Finally, we have `model.add(MaxPooling2D(pool_size=(2, 2)))`. A pooling layer take only the highest values of a section of the inputs. Essentialluy this is a downsampling function which only allows the most prominent features of the inputs to remain. In this pooling layer, we're passing a pool size of `2x2`, that is, we're dividing the images into quadrants and passing the maximum values found in those quandrants to the next layer of our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# First Group\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Higher Level Features\n",
    "\n",
    "Here, we repeat the stucture of our network. In our second group of layers, we have the same number of inputs as the output of our original layer of convolutional layer. Afterwards we have a second layer of activation nodes, followed by another pooling layer.\n",
    "\n",
    "By adding more neurons to our convolution layer in `model.add(Conv2D(64, (3, 3)))` we create in our network the possibility of exploring more higher level features. If we're looking for images of cats, we can imagine the first layers in the first group of layers of our neural network would have identified the edges of objects. The second and third groups of convolutional layer gives our network the opportunity to explore connections between different features identified in the first group of layers. So, as opposed to finding the edge of features in an image, the network can start to look at the relationships between those featurs identified in the first group of layers in our network. Going back to thinking about cats, it's a bit like being able to say \"This thing has ears (identification made in our first layer) but it also has four paws (exposed by the greater feature exploration enabled in our second layer)\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Group\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Third Group\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening Our Output\n",
    "\n",
    "At this point, after a number of epochs, our network will hopefully have built up enough information on the features in our spectrograns to adjust the weights which connect our neurons in a way that corresponds to an output that we expect - that is, it should be able to tell the difference between Stephen Colbert speaking and Conan O'Brien.\n",
    "\n",
    "In our fourth group of layers, we flatten our input (essentially create a one-dimensional array of numbers which we can feed into our next layer) and create a densely connected layer of neurons which have **ReLu** activation. This ensures that none of the neurons in the previous layers aren't ultimately connected to the subsequent series of layers in our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourth Group\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying our data\n",
    "\n",
    "Here we arrive at our final group of layers. First, we have a dropout layer `model.add(Dropout(0.5))`. When we speak of \"dropout\" in a neural network, we're talking about the process whereby neurons are randomly selected be ignored during the forward and backpropogation of data in our training stages. This has the effect of making our networks more robust to overfitting to our dataset, thus (hopefully) being able to better to classify inputs from more varied sources than that of our dataset. In this instance, we're dropping about half of the neurons from our network at random during training.\n",
    "\n",
    "Next we have a final densely connected layer `model.add(Dense(len(labelled_classes)))`. The number of neurons in this layers corresponds to the number of classes our dataset has. This is the place where a decision is effectively made on what label the input data should have in our neural network.\n",
    "\n",
    "Finally, we add a layer of activation neurons using a sigmoid function. The sigmoid function gives us a nice, curvy decider function that skirts around linearity. It inherently allows for nuance in deciding whether a thing is thing A or thing B based on the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fifth Group\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense( len(labelled_classes)))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling Our Model\n",
    "\n",
    "And that's it! We've constructed a very simple neural network to classify our spectrograms. Now it's time to compile our model to ready it for training.\n",
    "\n",
    "In our `model.compile` code, we pass an optimizer, a loss function, and the metric that we use to measure the accuracy of our network.\n",
    "\n",
    "The purpose of our optimiser is to best figure out how to update the weights of the connection between the neurons to minimise the loss function. Here, we're using the **ADAM** (**Ada**ptive **M**oment Estimation) optimiser.\n",
    "\n",
    "Next, we pass the loss function in this case we're using the `categorical_crossentropy` loss function to calculate the difference between the expected output and the actual output that our model produces. This loss function is generally used for networks where there are more than two classes to identify (in that case, we could use a `binary_crossentropy` loss function), but by using the `categorical_crossentropy` function, we can use the same code that can manage classification of more than two types of classes. For the loss function, smaller number is better.\n",
    "\n",
    "Finally, we have the metric that we use to measure the performance of our model. There can be a combination of metrics to measure the performance of our model, but here, we're only using `accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding our data into our model\n",
    "\n",
    "Now that we have our model compiled, we can start funelling our data into it to train it 🎉\n",
    "\n",
    "Keras has \"generators\" which can look at a directory structure, identify the classes therein, and then feed the data into our model.\n",
    "\n",
    "We need two generators, one for the feeding the training data into our model, the other for feeding the test data into our model.\n",
    "\n",
    "For this, we create the `train_datagen` and `test_datagen` variables. Our model needs to have values between 0 and 1 to function correctly, so when we call the `ImageDataGenerator` function to create the generator, we tell it to divide all of the values it finds in the file by `255` the maxiumum number a pixel in our images.\n",
    "\n",
    "In our `train_generator` and `test_generator`, we pass through the directories that Keras can find our divvied-up dataset in (remember when we unzipped that file _ages_ ago? Well, now we get to use it). \n",
    "\n",
    "In both of our generators, we pass a target size tuple. If we had large images in our dataset, this would rescale them to a more manageable size for our network to handle. We don't necessarily need a high resolution image for our model to work. In this case, our images are `110 x 110` pixels. This is small enough that we should have enough data for our model to figure out any patterns in our spectrograms, but small enough that the model should train within a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1914 images belonging to 2 classes.\n",
      "Found 360 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator( rescale = 1.0 / 255 )\n",
    "test_datagen = ImageDataGenerator(rescale = 1.0 / 255 )\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model\n",
    "\n",
    "And here we are, at the point where we get to train our model.\n",
    "\n",
    "If we hadn't used a generator to load our dataset, we could have called `model.fit` with a set of numpy arrays describing our data, but the generator is a nice way of quickly getting our model up and running. It's job is to break our training job into a smaller series of batches (a step) for training so that we don't have to load all of our data into memory at once. Though not a problem in this instance, large datasets may cause _out of memory_ errors when we try to train our model by overwhelming the systems capabilities. By breaking the training process up into smaller jobs, we can train our models on machines that don't neccesarily have an abundance of resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "59/59 [==============================] - 10s 161ms/step - loss: 0.6196 - accuracy: 0.6600 - val_loss: 0.1801 - val_accuracy: 0.9716\n",
      "Epoch 2/3\n",
      "59/59 [==============================] - 9s 158ms/step - loss: 0.2431 - accuracy: 0.9136 - val_loss: 0.0529 - val_accuracy: 0.9886\n",
      "Epoch 3/3\n",
      "59/59 [==============================] - 9s 158ms/step - loss: 0.0707 - accuracy: 0.9799 - val_loss: 0.0093 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x136a46d50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=nb_test_samples // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Our Model\n",
    "\n",
    "After our model has gone through however many epochs of training we've asked it to do, we can save it!\n",
    "\n",
    "By calling `model.save` we can pass a filename through to the function which will write a file. This will write the structure of the network as well as the weights between each neuron to a file, which makes it perfect for loading somewhere else. If you run the cell after `model.save` (`!ls -la`) you'll see that we now have the file `model.h5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reference = 'models/model_03.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m.\u001b[m\u001b[m           \u001b[34m..\u001b[m\u001b[m          model_01.h5 model_02.h5\r\n"
     ]
    }
   ],
   "source": [
    "!ls -a models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading + Predicting w. Our Model\n",
    "\n",
    "So, we've trained a model, but what good is that to us if we can't predict things with it? In this next cell, we'll load the model we just wrote to a file and use it to classify the data that we seperated from our dataset earlier on (the validation data).\n",
    "\n",
    "This is data that the model has never seen - not during training, and not during test - so if our model is any good, it should be able to pick out which of our two speakers are talking in each file.\n",
    "\n",
    "This code will get all of the files in the `validation` folder in our `data` folder and classify each one of them adding to a tally for each speaker.\n",
    "\n",
    "First, we identify all of the classes in our data structure with `labelled_class_names`, then we create a `tally` variable which will maintain a count of each correct and incorrectly identified speakers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a35e00edd4846009fe4e0d2671990fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e2a1c021ad46b8b0adb94c1e7660f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=60), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fefb406c4b5406681707c1d413326c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=60), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "{'colbert': {'correct': 60, 'incorrect': 0}, 'conan': {'correct': 58, 'incorrect': 2}}\n"
     ]
    }
   ],
   "source": [
    "stored_model = load_model(model_reference)\n",
    "\n",
    "labelled_class_names = [dI for dI in os.listdir('data/validation') \n",
    "                        if os.path.isdir(os.path.join('data/validation', dI ))]\n",
    "labelled_classes = {}\n",
    "\n",
    "tally = {}\n",
    "\n",
    "for class_name in tqdm(labelled_class_names):\n",
    "    \n",
    "    if class_name not in tally:\n",
    "        tally[class_name] = {\"correct\": 0, \"incorrect\": 0}\n",
    "\n",
    "    validationPath = 'data/validation/' + class_name\n",
    "\n",
    "    for f in tqdm(os.listdir(validationPath)):\n",
    "\n",
    "        if os.path.isfile(os.path.join(validationPath, f)) and f != \".DS_Store\":\n",
    "\n",
    "            filePath = validationPath + \"/\" + f\n",
    "\n",
    "            spectrogramFile = load_img(filePath, target_size=(110,110))\n",
    "            spectrogramFile = np.reshape(spectrogramFile, [1,110,110,3])\n",
    "            prediction = stored_model.predict_classes(spectrogramFile.astype(np.float16))\n",
    "\n",
    "            if class_name == \"colbert\" and prediction == 0:\n",
    "                tally[class_name][\"correct\"] += 1\n",
    "            elif class_name == \"colbert\" and prediction == 1:\n",
    "                tally[class_name][\"incorrect\"] += 1\n",
    "\n",
    "            if class_name == \"conan\" and prediction == 1:\n",
    "                tally[class_name][\"correct\"] += 1\n",
    "            elif class_name == \"conan\" and prediction == 0:\n",
    "                tally[class_name][\"incorrect\"] += 1\n",
    "print(tally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2] *",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
